{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0384be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_text\n",
      "['image_text']\n",
      "\n",
      "\n",
      "image_text\n",
      "['image_text']\n",
      "\n",
      "\n",
      "image_text\n",
      "['image_text']\n",
      "\n",
      "\n",
      "jond safra campus  \n",
      "['jond', 'safra', 'campus']\n",
      "\n",
      "\n",
      "= cor joh ee wing ok  lex  <{_jeeeneits |  alel xl \\s!] al] [finctonal =) &] 4] [es = @] [bex =] a] c: ms 3 jase |e [ins fine isne foons [eine s0ne josns one [isne sons [s@ne fsons sgne /7one sas poone  any 1b fc 1p fox 1fo2  pu ee  aestart| gy b® f)mal:: neox: avut...| @]foundation tutoval...[ 40 hwtpt -sosxtece... | $o schematic edtor -... | ap lagic mmulator - vi... | eb celusershielishty  te | [asa  \n",
      "['cor', 'joh', 'ee', 'wing', 'ok', 'lex', '_jeeeneits', 'alel', 'xl', 'al', 'finctonal', '4', 'es', 'bex', 'c', 'ms', '3', 'jase', 'e', 'ins', 'fine', 'isne', 'foons', 'eine', 's0ne', 'josns', 'one', 'isne', 'sons', 'ne', 'fsons', 'sgne', '7one', 'sas', 'poone', '1b', 'fc', '1p', 'fox', '1fo2', 'pu', 'ee', 'aestart', 'gy', 'b', 'f', 'mal', 'neox', 'avut', 'foundation', 'tutoval', '40', 'hwtpt', 'sosxtece', 'schematic', 'edtor', 'ap', 'lagic', 'mmulator', 'vi', 'eb', 'celusershielishty', 'te', 'asa']\n",
      "\n",
      "\n",
      "shut the» gin up  protect america  keep out of our fucking way liberal pussies.  \n",
      "['shut', 'gin', 'protect', 'america', 'keep', 'fuck', 'way', 'liberal', 'pussies']\n",
      "\n",
      "\n",
      "__ where  jb ony hl ag  meet miss and ‘coils  - a a  es a conference exploring the interface between ) contemporary body ideals jewish modesty laws  4 l j and eating disorders { d i‘ »  4 ip expert guest speakers lunch panel and q&a  sunday} wareh 20eh  12:00-3:00 pm steinhardt hall  \\ypos a1. 4 l>  ickets $5 until wes march 22nd; $7at the door  gister at www-hillel.upenn.edu/tci __ \n",
      "['__', 'jb', 'ony', 'hl', 'ag', 'meet', 'miss', 'coil', 'es', 'conference', 'explore', 'interface', 'contemporary', 'body', 'ideals', 'jewish', 'modesty', 'laws', '4', 'l', 'j', 'eat', 'disorder', '4', 'ip', 'expert', 'guest', 'speakers', 'lunch', 'panel', 'q', 'sunday', 'wareh', '20eh', '12', '00', '3', '00', 'pm', 'steinhardt', 'hall', 'ypos', 'a1', '4', 'l', 'ickets', '5', 'wes', 'march', '22nd', '7at', 'door', 'gister', 'www', 'hillel', 'upenn', 'edu', 'tci', '__']\n",
      "\n",
      "\n",
      "faveform viewer o} view window help ons) | [bek de orp [|| ene  z00ns 00n= 600ns je00ns plus pi.dus [dus piéus gus pus dus poaus totiohitiohotoltbb bbb  reform device options tools | al [fst =  ie on  us ga start| fo labsa -s10klpca4-4 - pro. eilaue amano manne 1: @rest 5:34pm \n",
      "['faveform', 'viewer', 'view', 'window', 'help', 'ons', 'bek', 'de', 'orp', 'ene', 'z00ns', '00n', '600ns', 'je00ns', 'plus', 'pi', 'dus', 'dus', 'piéus', 'gus', 'pus', 'dus', 'poaus', 'totiohitiohotoltbb', 'bbb', 'reform', 'device', 'options', 'tool', 'al', 'fst', 'ie', 'us', 'ga', 'start', 'fo', 'labsa', 's10klpca4', '4', 'pro', 'eilaue', 'amano', 'manne', '1', 'rest', '5', '34pm']\n",
      "\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed = open(\"../Data/processed_text.csv\", \"a+\", encoding=\"utf-8\")\n",
    "processed.write(\"Processed Text\\n\")\n",
    "\n",
    "with open(\"../Data/text_content.csv\", mode='r', encoding=\"utf-8\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 10:\n",
    "            break\n",
    "                \n",
    "        text = row[\"Image_Text\"].lower()\n",
    "        print(text)\n",
    "            \n",
    "        # Tokenization (alpha-numeric only)\n",
    "        tokenized = tokenizer.tokenize(text)\n",
    "        #print(tokenized)\n",
    "        \n",
    "        # Noise Removal (stop words)\n",
    "        tokens_no_stop = [word for word in tokenized if not word in stop_words]\n",
    "        #print(tokens_no_stop)\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word=word,pos='v') for word in tokens_no_stop]\n",
    "        print(lemmatized_words)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        line = f\"{lemmatized_words}\\n\"\n",
    "        processed.write(line)\n",
    "        \n",
    "        line_count += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
